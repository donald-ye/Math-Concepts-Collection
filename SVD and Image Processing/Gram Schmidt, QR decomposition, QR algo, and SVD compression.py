# -*- coding: utf-8 -*-
"""Project_5_Singular_Value_Decomposition_and_Image_Processing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lPTOjHy7CUYcGZqJAifZTyiaAS06c0ru

# Team Project 5 - Singular Value Decomposition and Image Processing

The Singular Value Decomposition (SVD) has many applications in pure mathematics, applied mathematics, and data science. A common theme of many applications of SVD is that for a matrix $A$, by using SVD, we can find a new matrix $A_k$ which is a good approximation of $A$, but the rank of $A_k$ is at most $k$. In general, a small rank matrix can be described with a lower number of entries; we can regard $A_k$ as a "compression" of $A$.

The goal of this project is twofold. First of all, we investigate how to compress image data using the already implemented SVD calculation method. Secondly, we will make a code for a few steps of the SVD calculation.

##### 1. (10 pts) Construct a method **GramSchmidt(A)** where $A = [\mathbf{v}_1, \mathbf{v}_2, \cdots, \mathbf{v}_n]$ is an invertible matrix, and its output is an orthogonal matrix $Q = [\mathbf{w}_1, \mathbf{w}_2, \cdots, \mathbf{w}_n]$ where its column vectors is an orthonormal basis obtained by applying the Gram-Schmidt process to $\{\mathbf{v}_1, \mathbf{v}_2, \cdots, \mathbf{v}_n\}$.
"""

import numpy as np

def GramSchmidt(A):
  n = A.shape[1] #.shape num of columns
  Q = np.zeros_like(A, dtype=float)

  # A[:, i] returns column i. A[i, :] return row
  for i in range(n):
    qi = A[:, i]
    for j in range(i):
      qj = Q[:, j]
      qi -= np.dot(qj, A[:, i]) * qj # subtracts projection
    Q[:, i] = qi / np.linalg.norm(qi) # normalize and store it in i-th column of Q
  return Q

A = np.array([[1, 1],
              [1, 0]],dtype=float)
GramSchmidt(A)

T = np.array([[1.,2.,3.],[4.,5.,6.],[2.,2.,1.]])
print(GramSchmidt(T))
# output of A should be

# Q = np.array([0.70710678  0.70710678],
#              [0.70710678 -0.70710678]])

"""2. (10 pts) Construct a method **QRdecomposition(A)** where $A$ is an invertible matrix and the output is a pair of matrices $[Q, R]$, that is the QR decomposition of $A$. In other words, $A = QR$ and $Q$ is an orthogonal matrix and $R$ is an upper triangular matrix."""

def QRdecomposition(A): #process is very similar to example above
  n = A.shape[1]
  Q = np.zeros_like(A, dtype=float)
  R = np.zeros((n, n), dtype=float) # upper traiangular

  for i in range(n):
    qi = A[:, i]
    for j in range(i):
      qj = Q[:, j]
      R[j, i] = np.dot(qj, A[:, i])
      qi -= R[j, i] * qj
    R[i, i] = np.linalg.norm(qi)
    Q[:, i] = qi / R[i,i]
  return Q, R

A = np.array([[1, 1],
              [1, 0]],dtype=float)
Q, R = QRdecomposition(A)

print("Orthogonal Matrix Q = \n", Q)
print("Upper Triangular Matrix R = \n", R)

print(QRdecomposition(T))

"""##### 3. (10 pts) A key step on the diagonalization of a symmetric matrix (and hence on SVD) is the QR method. Construct a method **QRalgorithm(A, err)** where $A$ is a symmetric tridiagonal matrix, err is a positive real number, and output is a list of eigenvalues of $A$. Let $A^{(k)}$ be the output of $k$-th iteration (See the notation in the lecture note) and let $e^{(k)}$ be the vector consisting of diagonal entries of $A^{(k)}$. Stop the iteration if either


*   $k = 1000$ or;
*   $||e^{(k)} - e^{(k-1)}||_{\infty} < \mathrm{err}$.

##### Let $M$ be a $(10 \times 10)$ symmetric tridiagonal matrix such that $$M_{ij} = \begin{cases}11-i, & \mbox{if } i = j,\\1, & \mbox{if } i = j+1 \mbox{ or } i = j-1,\\0, & \mbox{otherwise}.\end{cases}$$
##### By using **QRalgorithm(A, err)**, compute eigenvalues of $M$. Set $\mathrm{err} = 10^{-5}$.
"""

def generate_symmetric_tridiagonal_matrix(n): #creation of M
  M = np.zeros((n, n))
  for i in range(n):
    M[i, i] = 11 - i
    if i > 0:
      M[i, i - 1] = 1
    if i < n - 1:
      M[i, i + 1] = 1
  return M


def QRalgorithm(A, err):
  n = A.shape[0] #size of rows
  eig_old = np.diagonal(A)
  max_iterations = 1000

  for i in range(max_iterations):
    Q, R = QRdecomposition(A)
    A = np.dot(R, Q) #update A
    eig_new = np.diagonal(A)
    if np.linalg.norm(eig_new - eig_old, np.inf) < err: # break condition
      break

    eig_old = eig_new #update old eigenvalues
  return eig_new

M = generate_symmetric_tridiagonal_matrix(10)

print(M)
eigenvalues = QRalgorithm(M, 1e-5)

print("Eigenvalues in the matrix: \n", eigenvalues)

"""From now on, we will discuss image processing with Python. For simplicity, we are going to use a grayscale (black and white) image only. Below is how to convert a grayscale image to a python matrix. Matplotlib can only read the .png file natively."""

import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from PIL import Image
import requests
from io import BytesIO
import numpy as np
import numpy.linalg as la

response = requests.get('https://drive.google.com/uc?export=view&id=1Di59ub7nRPQRWpWZqaxWUzBRGnny8imU', stream=True)
sloth_img = Image.open(BytesIO(response.content))
sloth_img_array = np.array(sloth_img)[:,:,0]
# These three lines read 'sloth_gray.png' and record it as an array.

plt.imshow(sloth_img_array, cmap='gray')
# A matrix can be converted and shown to a grayscale image.

"""In the above code, <b>sloth_img_array</b> is a matrix. Thus we can compute its SVD and use it to do some image processing.

For a matrix $A$, <b>svd</b> method in the linear algebra module can be used to calculate its SVD. The output is a triplet of data $U$, $D$, and $V^t$, where $U$ and $V^t$ are orthogonal matrices and $D$ is a list of singular values of $A$ (not a diagonal matrix!). So if we denote $S$ as the diagonal matrix whose diagonal entries are numbers on $D$, then $A = USV^t$.
"""

A = np.array([[1,0,1],[0,1,0],[0,1,1],[0,1,0],[1,1,0]])
U, D, Vt = la.svd(A, full_matrices = True)
print("U=", U)
print("D=", D)
print("V^t=", Vt)

"""##### 4. (10 pts) Construct a method **SVDcompression(A, k)** which performs


*   Compute a SVD of $A$;
*   Calculate $A_k = U_k S_k V_k^t$, where $U_k$ is the first $k$ columns of $U$, $S_k$ is the first $k$ rows and $k$ columns of $S$, and $V_k^t$ is the first $k$ rows of $V^t$;
*   Convert $A_k$ as a grayscale image and show it.
"""

from scipy.linalg import svd
from skimage import io, color

def SVDcompression(A, k):
  U, S, Vt = svd(A)
  U_k = U[:, :k]        # k columns of U
  S_k = np.diag(S[:k])   # k singular values in a diagonal matrix
  Vt_k = Vt[:k, :]      # k rows of Vt

  A_k = np.dot(U_k, np.dot(S_k, Vt_k))

  plt.imshow(A_k, cmap='gray')
  plt.title(f'Compressed image with k={k} values')
  plt.show()

"""##### 5. (10 pts) Prepare your favorite image file in a grayscale .png format. (A photo is better than computer graphics. I suggest using a picture smaller than $500 \times 500$.) Load the image file and plot the original image. Convert the image as a matrix $A$. Run **SVDcompression(A,k)** for $k = 1, 5, 10$, and $50$."""

response = requests.get("https://drive.google.com/uc?export=download&id=1IsT5WO2-USQK-6F5UWidqTsKezj8BrUJ", stream=True)
favorite_img = Image.open(BytesIO(response.content))
favorite_img_array = np.array(favorite_img)[:,:,0]

plt.imshow(favorite_img_array, cmap='gray')

#original image in grayscale
plt.imshow(favorite_img_array, cmap='gray')
plt.title("Original Image")
plt.axis('off')
plt.show()

SVDcompression(favorite_img_array, k=1)
SVDcompression(favorite_img_array, k=5)
SVDcompression(favorite_img_array, k=10)
SVDcompression(favorite_img_array, k=50)

"""##### 6. (30 pts, extra credit) Create a method **SVDcalculation(A)** which computes the singular value decomposition of $A \in M_{m \times n}$ (with $m \ge n$) of a full rank matrix $A$ from scratch. Its output is a triple of matrices $[U, S, V]$ where
* $U$ is an $m \times m$ orthogonal matrix;
* $S$ is an $m \times n$ diagonal matrix with a positive decreasing diagonal entries;
* $V$ is an $n \times n$ orthogonal matrix;
* $A = USV^t$.

For the diagonalization of a symmetric matrix, use **QRalgorithm(A, err)** with $err = 10^{-5}$.

The only missing part of the SVD computation is the Householder reduction, which finds for a given symmetric matrix $A$ a similar tridiagonal matrix $B$. You may use the following command **hessenberg**. It returns two matrices $H$ and $Q$ such that
* $H$ is a tridiagonal matrix;
* $Q$ is an orthogonal matrix;
* $A = QHQ^t$.
"""

from scipy.linalg import hessenberg
A = np.array([[1,2,3,4],[2,5,6,7],[3,6,8,9],[4,7,9,10]])
print(A)
H, Q = hessenberg(A, calc_q=True)
# This command finds two matrices H, Q such that A = QH
print(H)
print(Q)
print(Q@H@(Q.T))

import numpy as np
from scipy.linalg import hessenberg

# simplified version
def QRalgorithm(A, err = 1e-5):
  n = A.shape[0]
  eig_old = np.diagonal(A)
  max_iterations = 1000

  for i in range(max_iterations):
    Q, R = np.linalg.qr(A)
    A = np.dot(R, Q)
    eig_new = np.diagonal(A)
    if np.linalg.norm(eig_new - eig_old, np.inf) < err:
        break
    eig_old = eig_new
  return eig_new, Q

def SVDcalculation(A):
  m, n = A.shape
  H, Q = hessenberg(A, calc_q=True)  # H is the tridiagonal matrix, Q is orthogonal matrix
  eigenvalues, eigenvectors = QRalgorithm(H)

  singular_values = np.sqrt(np.abs(eigenvalues))
  S = np.zeros((m, n))
  np.fill_diagonal(S, singular_values)

  # U is obtained by multiplying Q from the Householder reduction with the eigenvectors of H
  U = np.dot(Q, eigenvectors)
  V = eigenvectors

  return U, S, V

A = np.array([[1, 2, 3],
              [2 ,5 ,6],
              [3 ,6 ,8],
              [4 ,7 ,9]])

# SVD computation
U, S, V = SVDcalculation(A)
print("A matrix:")
print(A)
print("\nU matrix:")
print(U)
print("\nS matrix:")
print(S)
print("\nV matrix:")
print(V)

# Verification that A = USV^t
A_reconstructed = np.dot(U, np.dot(S, V.T))
print("\nReconstructed A:")
print(A_reconstructed)

# Answers: I'm getting cooked
# U matrix:
# [[-0.20739292  0.34827934 -0.92797369 -0.02220475]
#  [-0.46331061  0.70750698  0.4677419   0.14114043]
#  [-0.7192283   0.27869298  0.06571004 -0.64177527]
#  [-0.975146   -0.37112085 -0.29930304  0.1843296 ]]

# S matrix:
# [[17.99913415  0.          0.          0.        ]
#  [ 0.          1.25025102  0.          0.        ]
#  [ 0.          0.          0.13208361  0.        ]
#  [ 0.          0.          0.          0.01687902]]

# V matrix:
# [[-0.20739292 -0.46331061 -0.7192283  -0.975146  ]
#  [ 0.34827934  0.70750698  0.27869298 -0.37112085]
#  [-0.92797369  0.4677419   0.06571004 -0.29930304]
#  [-0.02220475  0.14114043 -0.64177527  0.1843296 ]]

# Reconstructed A:
# [[ 1.00000000e+00  2.00000000e+00  3.00000000e+00  4.00000000e+00]
#  [ 2.00000000e+00  5.00000000e+00  6.00000000e+00  7.00000000e+00]
#  [ 3.00000000e+00  6.00000000e+00  8.00000000e+00  9.00000000e+00]
#  [ 4.00000000e+00  7.00000000e+00  9.00000000e+00  1.00000000e+01]]

